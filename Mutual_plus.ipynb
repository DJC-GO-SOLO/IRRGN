{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0M2MqB86KQn",
    "outputId": "b63f6fca-d3e9-44bd-8c9a-39409547a50a"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y7SSOJJojMg7",
    "outputId": "cdabeea0-8d49-4a6d-e263-ffed9e09c8a2"
   },
   "outputs": [],
   "source": [
    "# !pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SAnwL4jccgVI",
    "outputId": "d5f4f533-2b76-4c65-fa88-9a671470acef"
   },
   "outputs": [],
   "source": [
    "# !pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_1MDz6oj_dG",
    "outputId": "717a602d-ea99-467f-dfe0-751665a8d158"
   },
   "outputs": [],
   "source": [
    "# !pip install torch-cluster -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-h6mOGZfkEIM",
    "outputId": "0e095a27-634b-449e-e0e6-6d1cc9872819"
   },
   "outputs": [],
   "source": [
    "# !pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ku7FF1pkK5g",
    "outputId": "a0e60581-e491-4884-87d4-d853dd9c1958"
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G7vDsjvbjVfe"
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# from transformers import pipeline\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "# nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "# example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "# ner_results = nlp(example)\n",
    "# print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1FgR8hDN2v4f"
   },
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './autodl-tmp/plus/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bdnDjYmxMx6A"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import wandb\n",
    "import datetime\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv,GATConv,GraphConv,GATv2Conv,RGATConv,RGCNConv\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from tqdm import tqdm, trange\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "from collections import defaultdict\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig,get_cosine_schedule_with_warmup,DebertaV2TokenizerFast\n",
    "#from modeling import (ElectraForMultipleChoicePlus, Baseline, BertBaseline, RobertaBaseline, BertForMultipleChoicePlus, RobertaForMultipleChoicePlus)\n",
    "from transformers import (get_linear_schedule_with_warmup, WEIGHTS_NAME, CONFIG_NAME)\n",
    "import re\n",
    "import os\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Random Seed Initialize\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def seed_everything(seed=RANDOM_SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPQUarxQOXgh",
    "outputId": "b4f7b282-7d16-499b-c37d-4d7dcd8d01f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found ( NVIDIA A100-SXM-80GB )\n",
      "num device avail:  1\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device Optimization\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"GPU found (\", torch.cuda.get_device_name(torch.cuda.current_device()), \")\")\n",
    "    print(\"num device avail: \", torch.cuda.device_count())\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    wandb=False\n",
    "    data_dir = \"input_plus\"\n",
    "    max_seq_length = 512\n",
    "    max_utterance_num = 30\n",
    "    model_name = \"microsoft/deberta-v2-xxlarge\"\n",
    "    epochs = 10\n",
    "    lr = 2e-6\n",
    "    output_dir= \"output\"\n",
    "    batch_size = 2\n",
    "    h_dim = 1536\n",
    "    max_grad_norm = 100\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    params = {\n",
    "    'scheduler_name': 'OneCycleLR',\n",
    "    'max_lr': 2e-6,                 # OneCycleLR\n",
    "    'pct_start': 0.1,               # OneCycleLR\n",
    "    'anneal_strategy': 'cos',       # OneCycleLR\n",
    "    'div_factor': 1e2,              # OneCycleLR\n",
    "    'final_div_factor': 1e2,        # OneCycleLR\n",
    "}\n",
    "    \n",
    "# def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "#     from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "#     logger = getLogger(__name__)\n",
    "#     logger.setLevel(INFO)\n",
    "#     handler1 = StreamHandler()\n",
    "#     handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "#     handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "#     handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "#     logger.addHandler(handler1)\n",
    "#     logger.addHandler(handler2)\n",
    "#     return logger\n",
    "\n",
    "# LOGGER = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Config.wandb:\n",
    "    \n",
    "    import wandb\n",
    "\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "        wandb.login(key='23a2d4b438741f956162624b0da65448022b5571')\n",
    "        anony = None\n",
    "    except:\n",
    "        anony = \"must\"\n",
    "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "\n",
    "    def class2dict(f):\n",
    "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "    run = wandb.init(project='EMNLP', \n",
    "                     name='test',\n",
    "                     config=class2dict(Config),\n",
    "                     group=Config.model_name,\n",
    "                     job_type=\"调参\",\n",
    "                     anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "3d-VLeE0cIEI"
   },
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, example_id, choices_features, label):\n",
    "        self.example_id = example_id\n",
    "        self.choices_features = [\n",
    "            {\n",
    "                'input_ids': input_ids,\n",
    "                'input_mask': input_mask,\n",
    "                'segment_ids': segment_ids,\n",
    "                'sep_pos': sep_pos,\n",
    "                'turn_ids': turn_ids,\n",
    "                'cls_pos':cls_pos\n",
    "            }\n",
    "            for input_ids, input_mask, segment_ids, sep_pos, turn_ids, cls_pos in choices_features\n",
    "        ]\n",
    "        self.label = label\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class MuTualProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the MuTual data set.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} train\".format(data_dir))\n",
    "        file = os.path.join(data_dir, 'train')\n",
    "        file = self._read_txt(file)\n",
    "        return self._create_examples(file, 'train')\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} dev\".format(data_dir))\n",
    "        file = os.path.join(data_dir, 'dev')\n",
    "        file = self._read_txt(file)\n",
    "        return self._create_examples(file, 'dev')\n",
    "\n",
    "    def get_test_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        logger.info(\"LOOKING AT {} test\".format(data_dir))\n",
    "        file = os.path.join(data_dir, 'test')\n",
    "        file = self._read_txt(file)\n",
    "        return self._create_examples(file, 'test')\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\", \"2\", \"3\"]\n",
    "\n",
    "    def _read_txt(self, input_dir):\n",
    "        lines = []\n",
    "        files = glob.glob(input_dir + \"/*txt\")\n",
    "        for file in tqdm(files, desc=\"read files\"):\n",
    "            with open(file, 'r', encoding='utf-8') as fin:\n",
    "                data_raw = json.load(fin)\n",
    "                data_raw[\"id\"] = file\n",
    "                lines.append(data_raw)\n",
    "        return lines\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (_, data_raw) in enumerate(tqdm(lines,desc=\"create examples\")):\n",
    "            id = \"%s-%s\" % (set_type, data_raw[\"id\"])\n",
    "            article = data_raw[\"article\"]\n",
    "\n",
    "            article = re.split(r\"(f : |m : |M: |F: )\", article)  #分割完是数组\n",
    "            #print(article)\n",
    "            article = [\"\".join(i) for i in zip(article[1::2], article[2::2])]  #然后再拼接好，最终的结果是每个说话者的话是列表中的一个元素\n",
    "\n",
    "            truth = str(ord(data_raw['answers']) - ord('A'))\n",
    "            options = data_raw['options']\n",
    "\n",
    "            examples.append(\n",
    "                InputExample(\n",
    "                    guid=id,\n",
    "                    text_a = [options[0], options[1], options[2], options[3]],\n",
    "                    text_b=article, # this is not efficient but convenient\n",
    "                    label=truth))\n",
    "        return examples\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length, max_utterance_num,\n",
    "                                 tokenizer, output_mode=None):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    \n",
    "    for (ex_index, example) in enumerate(tqdm(examples,desc=\"create features\")):\n",
    "        #if ex_index % 10000 == 0:\n",
    "        #    logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        choices_features = []\n",
    "        all_tokens = []\n",
    "        text_a = example.text_a\n",
    "        text_b = example.text_b\n",
    "\n",
    "        #for ending_idx, (text_a, text_b) in enumerate(zip(example.text_a, example.text_b)): #zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。\n",
    "        text_a[0] = re.sub(\"f : |m : |M: |F: \",\"\",text_a[0])\n",
    "        text_a[1] = re.sub(\"f : |m : |M: |F: \",\"\",text_a[1])\n",
    "        text_a[2] = re.sub(\"f : |m : |M: |F: \",\"\",text_a[2])\n",
    "        text_a[3] = re.sub(\"f : |m : |M: |F: \",\"\",text_a[3])\n",
    "        \n",
    "\n",
    "        tokens_a = tokenizer.tokenize(text_a[0])\n",
    "        tokens_a = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        tokens_b = tokenizer.tokenize(text_a[1])\n",
    "        tokens_b = [\"[CLS]\"] + tokens_b + [\"[SEP]\"]\n",
    "        tokens_c = tokenizer.tokenize(text_a[2])\n",
    "        tokens_c = [\"[CLS]\"] + tokens_c + [\"[SEP]\"]\n",
    "        tokens_d = tokenizer.tokenize(text_a[3])\n",
    "        tokens_d = [\"[CLS]\"] + tokens_d\n",
    "        tokens_options = tokens_a + tokens_b + tokens_c + tokens_d\n",
    "\n",
    "        tokens_article = []\n",
    "\n",
    "        for idx, text in enumerate(text_b):  \n",
    "            if len(text.strip()) > 0:  #strip() 方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列。\n",
    "                text = re.sub(\"f : |m : |M: |F: \",\"\",text)\n",
    "                tokens_article.extend([\"[CLS]\"]+tokenizer.tokenize(text) + [\"[SEP]\"])  #extend() 函数用于在列表末尾一次性追加另一个序列中的多个值（用新列表扩展原来的列表）。\n",
    "                                                #而append是加入一个对象，如果这里用append的话那么可能会变成将列表加入到原来的列表当中\n",
    "        tokens_article.pop(0)\n",
    "        _truncate_seq_pair(tokens_options, tokens_article, max_seq_length-2)\n",
    "\n",
    "        tokens = [\"[CLS]\"]\n",
    "        turn_ids = [0]\n",
    "\n",
    "        context_len = []\n",
    "        sep_pos = []\n",
    "        cls_pos = [0]\n",
    "\n",
    "            \n",
    "        tokens_article_raw = \" \".join(tokens_article)\n",
    "        tokens_article = []\n",
    "        current_pos = 0\n",
    "        for toks in tokens_article_raw.split(\"[SEP]\")[-max_utterance_num - 1:-1]:\n",
    "            context_len.append(len(toks.split()) + 1)\n",
    "            tokens_article.extend(toks.split())\n",
    "            tokens_article.extend([\"[SEP]\"])\n",
    "            current_pos += context_len[-1]\n",
    "            turn_ids += [len(sep_pos)] * context_len[-1]\n",
    "            sep_pos.append(current_pos)\n",
    "            cls_pos.append(current_pos+1)\n",
    "        cls_pos.pop()\n",
    "                \n",
    "        tokens += tokens_article\n",
    "\n",
    "        segment_ids = [0] * (len(tokens))\n",
    "\n",
    "        tokens_options += [\"[SEP]\"]\n",
    "        \n",
    "\n",
    "        for index,toks in enumerate(tokens_options):\n",
    "          if toks == \"[CLS]\":\n",
    "            cls_pos.append(len(tokens)+index)\n",
    "\n",
    "        tokens += tokens_options\n",
    "\n",
    "        segment_ids += [1] * (len(tokens_options))\n",
    "            \n",
    "        turn_ids += [len(sep_pos)] * len(tokens_options) \n",
    "        sep_pos.append(len(tokens) - 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "        turn_ids += padding\n",
    "\n",
    "        context_len += [-1] * (max_utterance_num - len(context_len))\n",
    "        sep_pos += [0] * (max_utterance_num + 1 - len(sep_pos))\n",
    "        num_nodes = len(cls_pos)\n",
    "        cls_pos += [0] * (max_utterance_num + 1 - len(cls_pos))\n",
    "        cls_pos[-1] = num_nodes\n",
    "\n",
    "        assert len(sep_pos) == max_utterance_num + 1\n",
    "        assert len(cls_pos) == max_utterance_num + 1\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(context_len) == max_utterance_num \n",
    "        assert len(turn_ids) == max_seq_length \n",
    "\n",
    "        choices_features.append((input_ids, input_mask, segment_ids, sep_pos, turn_ids, cls_pos))   #turn_ids代表每个说话者句子的序号，\n",
    "            #最前面的是历史轮对话，最后面的是响应答案和填充；sep_pos代表语句的结束位置；segment_ids代表历史论对话和响应答案，0，1，\n",
    "            #input_mask代表有效的和填充的，1，0\n",
    "        all_tokens.append(tokens)\n",
    "\n",
    "\n",
    "        label_id = label_map[example.label] #数字型\n",
    "        \n",
    "\n",
    "        if ex_index < 10:  #打印一下输出\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            for choice_idx, (input_ids, input_mask, segment_ids, sep_pos, turn_ids, cls_pos) in enumerate(choices_features):\n",
    "                logger.info(\"choice: {}\".format(choice_idx))\n",
    "                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "                logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "                logger.info(\"tokens: %s\" % \" \".join([str(x) for x in all_tokens[choice_idx]]))\n",
    "                logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "                logger.info(\"sep_pos: %s\" % \" \".join([str(x) for x in sep_pos]))\n",
    "                logger.info(\"turn_ids: %s\" % \" \".join([str(x) for x in turn_ids]))\n",
    "                logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                example_id = example.guid, \n",
    "                choices_features = choices_features,\n",
    "                label = label_id\n",
    "                )\n",
    "        )\n",
    "\n",
    "    return features\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "40d5832f202b44c89d8dd37c676b52bb",
      "df818c9657b343dc8cd3aa7a1a23e991",
      "fa1784d71c9c48ad844309aceda59def",
      "2f2a9197594c4a8890cd2485958e80f6",
      "ee22a71cb96549b09ec6296ef758fb08",
      "53384bab4052401a93010f763b6f5c64",
      "144981735bd14cbf86592f9842a8c0f1",
      "9e7bb7cf4f694ab9aaf1d1b8e290f368",
      "0c00484cabf84e2eb36826f4b7d1aa6d",
      "3a824b6693c4422da00e6a64a51a012b",
      "d34174ad7a414212a62a84a386ab0f70",
      "e1b06acc73bb4aa9b5a785674146805b",
      "ee4355137e1e40a49bf6af5d56ecad6b",
      "abe66687a433455d96ef6ea193d1993c",
      "a2c4ea5c01a64723ab2f415b2bb8ba57",
      "99cfd3bb5d534331a223d5faa93b244f",
      "a5d2592adfda4df9b6939e9a30a4c913",
      "48bf8cc47df04c128bf39221d8dbd69a",
      "d5490e6b9a124714adba145ca2bb85f4",
      "b606de690798410ab480bdb5d56fca70",
      "3f466f3bbb684977a6b4fdf47930b8f9",
      "42663a30a68043f2978fcfe3712b84fa",
      "5521f879c1ad42dda99c46c152845629",
      "89f84f6a2dd946e6ac0cb164f223d065",
      "944f758a16164c4f87d551f4fe85755b",
      "ccb325512a7344dab2328cd4267dd1c1",
      "91337ecf49244217ba63ac92da707e97",
      "8e8bbe367b4343a2afc2296821e9fe73",
      "8b904cba9ef4498f97c9cadfdcb9801f",
      "c9001aebde3d4663b3b29a7bddef3ae5",
      "69beae0cf0c0440783d5d5fdd8f41bfd",
      "acf26465e1aa44239c3cec382b4b33d5",
      "fc1b5f1cf144419b8b5dd4dc3dc2af4b",
      "3dcb2744e11246bdb409c03ef6929a08",
      "3e5185c08d4c4709a293ec01fa2d598d",
      "8b46d71efaac449d9f1b8eaea7d7437f",
      "3da2472043fb403a8baca25a84c3fbfc",
      "d4d620025a2149cf84154718f0f3a889",
      "1e6c446a710b4ded9aa3dac0c836d295",
      "de70481d4882465ebe6f313940b9a093",
      "a475b7c855bf45b48dad5cfd7f8814a7",
      "4f7745e2b98542d88c0003641e02ceb3",
      "2f63c480d1574a69ad03d7d4a1d7417e",
      "a2f96386fe184f38ad74700130141290"
     ]
    },
    "id": "sC3YQdE19mEX",
    "outputId": "f63e45f8-ae4f-472c-fb10-fa5324db4b50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(Config.model_name)\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained(Config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFBY4fKCd5Mp",
    "outputId": "ad230069-1101-4ca9-bcfa-5944ec0fa437"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read files: 100%|██████████| 7088/7088 [00:00<00:00, 34287.87it/s]\n",
      "create examples: 100%|██████████| 7088/7088 [00:00<00:00, 96672.24it/s]\n"
     ]
    }
   ],
   "source": [
    "processor = MuTualProcessor()\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "train_examples = processor.get_train_examples(Config.data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FE-o6nrQ0w4J",
    "outputId": "a03d673f-a6b7-4147-f951-96fd53c18a32"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read files: 100%|██████████| 886/886 [00:00<00:00, 32082.27it/s]\n",
      "create examples: 100%|██████████| 886/886 [00:00<00:00, 100084.93it/s]\n"
     ]
    }
   ],
   "source": [
    "val_examples = processor.get_dev_examples(Config.data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FUX-umne9qfb",
    "outputId": "70481305-c0f1-4bf5-e762-05ad48011c79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "create features: 100%|██████████| 7088/7088 [00:07<00:00, 967.43it/s] \n"
     ]
    }
   ],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "                train_examples, label_list, Config.max_seq_length, Config.max_utterance_num, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9UYnnr2M1GKO",
    "outputId": "19b8eaf3-7898-4c1a-998d-1a0eb6b6a310"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "create features: 100%|██████████| 886/886 [00:00<00:00, 1001.26it/s]\n"
     ]
    }
   ],
   "source": [
    "val_features = convert_examples_to_features(\n",
    "                val_examples, label_list, Config.max_seq_length, Config.max_utterance_num, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "mAwJiiFAv_pk"
   },
   "outputs": [],
   "source": [
    "class MuTualDataset(Dataset):\n",
    "  def __init__(self, features):\n",
    "    self.features = features\n",
    "    self.length = len(features)\n",
    "\n",
    "  def __len__(self): \n",
    "      return self.length\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    input_ids = self.features[idx].choices_features[0]['input_ids']\n",
    "    input_mask = self.features[idx].choices_features[0]['input_mask']\n",
    "    segment_ids = self.features[idx].choices_features[0]['segment_ids']\n",
    "    cls_pos = self.features[idx].choices_features[0]['cls_pos']\n",
    "    label = self.features[idx].label\n",
    "    \n",
    "\n",
    "    label_id = np.zeros((4,1),dtype=int)\n",
    "\n",
    "    label_id[label,0] = 1\n",
    "    #length = len(cls_pos)\n",
    "    \n",
    "    #edge_index = _get_edge_index(length, edge_index)\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
    "    input_mask = torch.tensor(input_mask, dtype=torch.long)\n",
    "    segment_ids =torch.tensor(segment_ids, dtype=torch.long)\n",
    "    cls_pos = torch.tensor(cls_pos, dtype=torch.long)\n",
    "    label_id = torch.tensor(label_id, dtype=torch.float)\n",
    "\n",
    "    return {\"input_ids\":input_ids,\"input_mask\": input_mask,\"segment_ids\": segment_ids,\"cls_pos\":cls_pos,\"label_id\":label_id}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mGj4l3kJFrSl"
   },
   "outputs": [],
   "source": [
    "def get_edge_index(length):\n",
    "    all_edges = set()\n",
    "    all_options_edges = set()\n",
    "    num_edges = (length-4)*(length-5) + (length-4)*4\n",
    "    #edge_index=numpy.zeros((2,num_labels,dtype=int)\n",
    "\n",
    "    for i in range(length-4,length):\n",
    "      for j in range(length-4,length):\n",
    "          all_options_edges.add((i,j))\n",
    "  \n",
    "    #cnt=0\n",
    "    for i in range(length-4):\n",
    "      for j in range(length-4):\n",
    "        if i != j:\n",
    "          all_edges.add((i,j))\n",
    "    for i in range(length-4):\n",
    "      for j in range(length-4,length):\n",
    "          all_edges.add((i,j))\n",
    "    \n",
    "    assert num_edges == len(all_edges)\n",
    "\n",
    "    return list(all_edges),len(all_edges),list(all_options_edges)\n",
    "\n",
    "\n",
    "def batch_graphify(batch_output,batch_cls_pos):\n",
    "  #edge_length_sum = 0\n",
    "  batch_size = len(batch_output)\n",
    "  #print(\"batch_output:\",batch_output.shape)\n",
    "  #print(\"batch_cls_pos:\",batch_cls_pos.shape)\n",
    "\n",
    "  node_length_sum = 0\n",
    "  edge_index_batch = []\n",
    "  options_edge_index_batch = []\n",
    "  nodes_feature_batch_first = batch_output[:,0,:] #4,768\n",
    "  #print(\"nodes_feature_batch_first:\",nodes_feature_batch_first)\n",
    "  nodes_feature_list = []\n",
    "  options_cls_batch = []\n",
    "  for i in range(batch_size):\n",
    "    \n",
    "      \n",
    "    edges,edges_length,options_edges = get_edge_index(batch_cls_pos[i,-1])\n",
    "    #print(\"edges:\",edges)\n",
    "    edges_s = [(item[0]+node_length_sum, item[1]+node_length_sum) for item in edges]\n",
    "    #print(\"edges_s:\",edges_s)\n",
    "    for item in edges_s:\n",
    "      edge_index_batch.append(torch.tensor([item[0], item[1]]))\n",
    "    options_edges_s = [(item[0]+node_length_sum, item[1]+node_length_sum) for item in options_edges]\n",
    "    #print(\"edges_s:\",edges_s)\n",
    "    for item in options_edges_s:\n",
    "      options_edge_index_batch.append(torch.tensor([item[0], item[1]]))\n",
    "\n",
    "\n",
    "    #edge_length_sum+=edges_length\n",
    "    node_length_sum+=batch_cls_pos[i,-1]\n",
    "    #print(\"node_length_sum:\",node_length_sum)\n",
    "    \n",
    "    nodes_feature = nodes_feature_batch_first[i].unsqueeze(0) #1,768\n",
    "    #print(\"nodes_feature:\",nodes_feature.shape)\n",
    "    for j in range(len(batch_cls_pos[i])-1):\n",
    "      if batch_cls_pos[i,j] != 0:\n",
    "        nodes_feature = torch.cat((nodes_feature,batch_output[i,batch_cls_pos[i,j]].unsqueeze(0)),0)\n",
    "        #print(nodes_feature)\n",
    "    nodes_feature_list.append(nodes_feature)\n",
    "    for k in range(-4,0):\n",
    "      options_cls_batch.append(node_length_sum+k)\n",
    "    \n",
    "\n",
    "  \n",
    "  nodes_feature_batch = nodes_feature_list[0]\n",
    "  for i in range(len(nodes_feature_list)):\n",
    "    if i !=0:\n",
    "      nodes_feature_batch = torch.cat((nodes_feature_batch,nodes_feature_list[i]),0)\n",
    "\n",
    "  nodes_feature_batch = nodes_feature_batch.to(device)\n",
    "  edge_index_batch = torch.stack(edge_index_batch).transpose(0, 1).to(device)\n",
    "  options_edge_index_batch = torch.stack(options_edge_index_batch).transpose(0, 1).to(device)\n",
    "\n",
    "  options_cls_batch = torch.tensor(options_cls_batch).to(device)\n",
    "  #print(\"nodes_feature_batch:\",nodes_feature_batch.shape)\n",
    "  #print(\"edge_index_batch:\",edge_index_batch)\n",
    "  #print(\"options_cls_batch:\",options_cls_batch)\n",
    "\n",
    "\n",
    "\n",
    "  return nodes_feature_batch,edge_index_batch,options_cls_batch,options_edge_index_batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VCZIDCuyyIUE"
   },
   "outputs": [],
   "source": [
    "class MetricMonitor:\n",
    "    def __init__(self, float_precision=4):\n",
    "        self.float_precision = float_precision\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})#defaultdict的作用是在于，当字典里的key不存在但被查找时，返回的不是keyError而是一个默认值\n",
    "\n",
    "    def update(self, metric_name, val):\n",
    "        metric = self.metrics[metric_name]\n",
    "\n",
    "        metric[\"val\"] += val\n",
    "        metric[\"count\"] += 1\n",
    "        metric[\"avg\"] = metric[\"val\"] / metric[\"count\"]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \" | \".join(\n",
    "            [\n",
    "                \"{metric_name}: {avg:.{float_precision}f}\".format(\n",
    "                    metric_name=metric_name, avg=metric[\"avg\"],\n",
    "                    float_precision=self.float_precision\n",
    "                )\n",
    "                for (metric_name, metric) in self.metrics.items()\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "P0AjUjhuyrMD"
   },
   "outputs": [],
   "source": [
    "class Co_attention_head(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Co_attention_head, self).__init__()\n",
    "        self.q1 = nn.Linear(Config.h_dim,384)\n",
    "        self.q2 = nn.Linear(Config.h_dim,384)\n",
    "        self.q3 = nn.Linear(Config.h_dim,384)\n",
    "        self.q4 = nn.Linear(Config.h_dim,384)\n",
    "    \n",
    "        self.k1 = nn.Linear(Config.h_dim,384)\n",
    "        self.k2 = nn.Linear(Config.h_dim,384)\n",
    "        self.k3 = nn.Linear(Config.h_dim,384)\n",
    "        self.k4 = nn.Linear(Config.h_dim,384)\n",
    "        \n",
    "        self.v1 = nn.Linear(Config.h_dim,384)\n",
    "        self.v2 = nn.Linear(Config.h_dim,384)\n",
    "        self.v3 = nn.Linear(Config.h_dim,384)\n",
    "        self.v4 = nn.Linear(Config.h_dim,384)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(Config.h_dim)\n",
    "    def forward(self, node1,node2):\n",
    "        q1 = self.q1(node1)\n",
    "        q2 = self.q2(node1)\n",
    "        q3 = self.q3(node1)\n",
    "        q4 = self.q4(node1)\n",
    "        \n",
    "        k1 = self.k1(node2)\n",
    "        k2 = self.k2(node2)\n",
    "        k3 = self.k3(node2)\n",
    "        k4 = self.k4(node2)\n",
    "        \n",
    "        v1 = self.v1(node2)\n",
    "        v2 = self.v2(node2)\n",
    "        v3 = self.v3(node2)\n",
    "        v4 = self.v4(node2)\n",
    "        \n",
    "        r1 = (torch.matmul(q1,k1.permute(1,0))/((Config.h_dim/4)**0.5))*v1\n",
    "        r2 = (torch.matmul(q2,k2.permute(1,0))/((Config.h_dim/4)**0.5))*v2\n",
    "        r3 = (torch.matmul(q3,k3.permute(1,0))/((Config.h_dim/4)**0.5))*v3\n",
    "        r4 = (torch.matmul(q4,k4.permute(1,0))/((Config.h_dim/4)**0.5))*v4\n",
    "        \n",
    "        return self.layer_norm(torch.cat((r1,r2,r3,r4),-1)) \n",
    "\n",
    "class MuTualModel(nn.Module):\n",
    "  def __init__(self,model_name=Config.model_name):\n",
    "    super(MuTualModel, self).__init__()\n",
    "    self.model_name = model_name\n",
    "    self.config = AutoConfig.from_pretrained(model_name)\n",
    "    self.model = AutoModel.from_pretrained(model_name,config=self.config)\n",
    "    #self.conv1 = GATConv(768, 768,dropout=0.2)\n",
    "    #self.conv2 = GCNConv(768, 768)\n",
    "    # self.conv2 = GATConv(768, 768,dropout=0.2)\n",
    "    #self.conv2 = GCNConv(768, 768)\n",
    "    self.conv3 = GraphConv(768,64)\n",
    "    self.conv1 = RGCNConv(Config.h_dim,768,8)\n",
    "    self.encoder_layer1 = nn.TransformerEncoderLayer(d_model=Config.h_dim, nhead=2,batch_first=True)\n",
    "    self.encoder_layer2 = nn.TransformerEncoderLayer(d_model=64, nhead=2,batch_first=True)\n",
    "    self.linear1 = nn.Linear(64, 64)\n",
    "    self.linear2 = nn.Linear(64, 1)\n",
    "    self.tanh = nn.Tanh()\n",
    "    self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    self.coattention =Co_attention_head()\n",
    "    #self.linear3 = nn.Linear(768*2,768)\n",
    "    self.linear4 = nn.Linear(Config.h_dim,Config.h_dim)\n",
    "    self.linear5 = nn.Linear(Config.h_dim,64)\n",
    "    self.linear6 = nn.Linear(64,8)\n",
    "\n",
    "    self.relu = nn.ReLU()\n",
    "    self.softmax1 = nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "  \n",
    "  def forward(self, input_ids, input_mask,segment_ids, cls_pos):\n",
    "    outputs = self.model(input_ids,attention_mask=input_mask,token_type_ids=segment_ids)\n",
    "    sequence_output = outputs[0]  #拿出最后一层\n",
    "    #print(\"sequence_output:\",sequence_output.shape)\n",
    "\n",
    "    \n",
    "    nodes_feature_batch,edge_index_batch,options_cls_batch,options_edge_index_batch = batch_graphify(sequence_output,cls_pos)\n",
    "    batch_size = len(sequence_output)\n",
    "    options_batch_raw = torch.zeros((batch_size*4,Config.h_dim),dtype = torch.float32).to(device)\n",
    "    for index,i in enumerate(options_cls_batch):\n",
    "      options_batch_raw[index] = nodes_feature_batch[i]\n",
    "    options_batch_raw = options_batch_raw.view(batch_size,4,Config.h_dim)\n",
    "    #print(nodes_feature_batch)\n",
    "    #print(options_batch_raw)\n",
    "    options_batch_mutual = self.encoder_layer1(options_batch_raw)\n",
    "   \n",
    "    options_batch_mutual = options_batch_mutual.view(batch_size*4,Config.h_dim)\n",
    "\n",
    "    for index,i in enumerate(options_cls_batch):\n",
    "      nodes_feature_batch[i] = options_batch_mutual[index]\n",
    "\n",
    "\n",
    "\n",
    "    #print(nodes_feature_batch)\n",
    "    #print(nodes_feature_batch.shape)\n",
    "    #print(edge_index_batch.shape)\n",
    "    #print(options_cls_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(sequence_output.shape)\n",
    "    #print(cls_pos.shape)\n",
    "\n",
    "    #nodes_feature = sequence_output[:,0]\n",
    "    # #print(nodes_feature.shape)\n",
    "\n",
    "    # for i in range(len(cls_pos[0,:])-1):\n",
    "    #   if cls_pos[0,i] != 0:\n",
    "    #     nodes_feature = torch.cat((nodes_feature,sequence_output[:,cls_pos[0,i]]),0)\n",
    "    \n",
    "    #print(nodes_feature.shape)\n",
    "    #print(nodes_feature)\n",
    "    #print(cls_pos)\n",
    "    \n",
    "    # length = cls_pos[:,-1].item()\n",
    "    # num_edges = (length-4)*(length-5) + (length-4)*4\n",
    "    \n",
    "    # edge_index=torch.zeros((2,num_edges),dtype=torch.long).to(device)\n",
    "    # cnt=0\n",
    "    # for i in range(length-4):\n",
    "    #   for j in range(length-4):\n",
    "    #     if i != j:\n",
    "    #       edge_index[0,cnt] = i\n",
    "    #       edge_index[1,cnt] = j\n",
    "    #       cnt= cnt+1\n",
    "    # for i in range(length-4):\n",
    "    #   for j in range(length-4,length):\n",
    "    #       edge_index[0,cnt] = i\n",
    "    #       edge_index[1,cnt] = j\n",
    "    #       cnt= cnt+1\n",
    "\n",
    "    #nodes_feature = nodes_feature.unsqueeze(0)\n",
    "    #print(nodes_feature.shape)\n",
    "    #print(nodes_feature_batch[-4:,:])\n",
    "    #output = self.conv1(nodes_feature_batch,options_edge_index_batch)\n",
    "    #print(output[-4:,:])\n",
    "    #print(output) \n",
    "    #print(output.shape)\n",
    "    edge_type = torch.zeros(edge_index_batch.size(1),dtype=torch.long).to(device)\n",
    "\n",
    "    for i in range(edge_index_batch.size(1)):\n",
    "      start = edge_index_batch[0,i]\n",
    "      end = edge_index_batch[1,i]\n",
    "      temp = self.relu(self.coattention(nodes_feature_batch[start].unsqueeze(0),nodes_feature_batch[end].unsqueeze(0)))\n",
    "      #temp = torch.cat((word_nodes_feature_batch_a[start],word_nodes_feature_batch_a[end]),-1)\n",
    "      #temp = self.tanh(self.linear3(temp))\n",
    "      temp = self.relu(self.linear4(temp))\n",
    "      temp = self.relu(self.linear5(temp))\n",
    "      result = self.softmax1(self.linear6(temp))\n",
    "      edge_type[i]=torch.argmax(result,dim=1)\n",
    "    \n",
    "    # edge_index_batch = edge_index_batch[:,edge_type != 8]\n",
    "    # edge_type = edge_type[edge_type != 8]\n",
    "\n",
    "    output = self.conv1(nodes_feature_batch,edge_index_batch,edge_type)\n",
    "    output = self.conv3(output,edge_index_batch)\n",
    "    #print(output[-4:].shape)\n",
    "    #print(output)\n",
    "    #print(output.shape)\n",
    "\n",
    "    options_batch = torch.zeros((batch_size*4,64),dtype = torch.float32).to(device)\n",
    "    cnt = 0\n",
    "    for i in options_cls_batch:\n",
    "      options_batch[cnt] = output[i]\n",
    "      cnt+=1\n",
    "    #print(options_batch)\n",
    "    options_batch = options_batch.view(batch_size,4,64)\n",
    "    #print(output)\n",
    "    #print(options_batch)\n",
    "    options_batch = self.encoder_layer2(options_batch)\n",
    "    options_batch = self.tanh(self.linear1(options_batch))\n",
    "    #print(options_batch.shape)\n",
    "    options_batch = self.linear2(options_batch)\n",
    "    #print(options_batch)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # logits = self.softmax(options_batch)\n",
    "    #print(logits.shape)\n",
    "    #print(logits)\n",
    "\n",
    "    return options_batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "uGiA5ATEOAQf"
   },
   "outputs": [],
   "source": [
    "def criterion2(preds, labels):\n",
    "  p1=0\n",
    "  p2 = 0\n",
    "  mrr = 0\n",
    "  for i in range(len(preds)):\n",
    "    j = sorted(list(preds[i]), reverse = True)\n",
    "    for index,label in enumerate(labels[i]):\n",
    "      if label == 1:\n",
    "        if preds[i,index] == j[0]:\n",
    "          p1+=1\n",
    "          p2+=1\n",
    "          mrr += 1 \n",
    "          break\n",
    "        elif preds[i,index] == j[1]:\n",
    "          p2+=1\n",
    "          mrr += 1/2\n",
    "          break\n",
    "        elif preds[i,index] == j[2]:\n",
    "          mrr += 1/3\n",
    "        elif preds[i,index] == j[3]:\n",
    "          mrr += 1/4\n",
    "    \n",
    "  return p1 / len(preds), p2 / len(preds), mrr / len(preds)\n",
    "\n",
    "\n",
    "# def criterion1(preds, targets):\n",
    "#   return nn.BCELoss()(preds,targets)\n",
    "def criterion1(preds, targets):\n",
    "  return nn.CrossEntropyLoss(label_smoothing=0.05)(preds,targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MDwlgyzTOBXd"
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion1, optimizer ,scheduler=None):\n",
    "    metric_monitor = MetricMonitor()\n",
    "    model.train()\n",
    "    stream = tqdm(train_loader)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "    \n",
    "    for step,batch in enumerate(stream):\n",
    "        \n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        input_mask = batch[\"input_mask\"].to(device)\n",
    "        segment_ids = batch[\"segment_ids\"].to(device)\n",
    "        cls_pos = batch[\"cls_pos\"].to(device)\n",
    "        label = batch[\"label_id\"].to(device)\n",
    "        #edge_index = batch[\"edge_index\"].to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=True):\n",
    "            preds = model(input_ids,input_mask,segment_ids,cls_pos)\n",
    "\n",
    "        #print(preds)\n",
    "        #print(targets)\n",
    "        #print(preds.shape)\n",
    "        #print(label)\n",
    "        loss = criterion1(preds, label)\n",
    "        #print(loss.item())\n",
    "        metric_monitor.update('Loss', loss.item())\n",
    "\n",
    "        # loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        # grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), Config.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        stream.set_description(f\"Epoch: {epoch:02}. Train. {metric_monitor}\")\n",
    "        # wandb.log({f\"loss\": loss.item()})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "b7UIzRmxgi27"
   },
   "outputs": [],
   "source": [
    "def validate_fn(val_loader, model, criterion2):\n",
    "    metric_monitor1 = MetricMonitor()\n",
    "    metric_monitor2 = MetricMonitor()\n",
    "    metric_monitor3 = MetricMonitor()\n",
    "    #results =[]\n",
    "    model.eval()\n",
    "    stream = tqdm(val_loader)\n",
    "    all_r4_1 = []\n",
    "    all_r4_2=[]\n",
    "    all_mrr=[]\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(stream ):\n",
    "          input_ids = batch[\"input_ids\"].to(device)\n",
    "          input_mask = batch[\"input_mask\"].to(device)\n",
    "          segment_ids = batch[\"segment_ids\"].to(device)\n",
    "          cls_pos = batch[\"cls_pos\"].to(device)\n",
    "          label = batch[\"label_id\"].to(device)\n",
    "          \n",
    "\n",
    "          preds = model(input_ids,input_mask,segment_ids,cls_pos)\n",
    "          #for i in range(len(preds)):\n",
    "          #  results.append(np.argmax(preds.cpu().numpy(),axis=1))\n",
    "          #print(\"label\",label)\n",
    "          #print(\"preds:\",preds)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "          r4_1,r4_2,mrr = criterion2(preds,label)\n",
    "\n",
    "          #print(\"r4_1:\",r4_1)\n",
    "          #print(\"r4_2:\",r4_2)\n",
    "          #print(\"mrr:\",mrr)\n",
    "          \n",
    "          all_r4_1.append(r4_1)\n",
    "          all_r4_2.append(r4_2)\n",
    "          all_mrr.append(mrr)\n",
    "          metric_monitor1.update('R4_1', r4_1)\n",
    "          metric_monitor2.update('R4_2', r4_2)\n",
    "          metric_monitor3.update('MRR', mrr)\n",
    "          stream.set_description(f\"Epoch: {epoch:02}. Valid. {metric_monitor1} {metric_monitor2} {metric_monitor3}\")\n",
    "          # wandb.log({\"R4_1\": r4_1,\n",
    "          #           \"R4_2\": r4_2,\n",
    "          #           \"MRR\": mrr})\n",
    "          \n",
    "    #print(results)\n",
    "    # LOGGER.info(f'R4_1:{(np.mean(all_r4_1))},R4_2:{(np.mean(all_r4_2))},MRR:{(np.mean(all_mrr))}')\n",
    "    return np.mean(all_r4_1),np.mean(all_r4_2),np.mean(all_mrr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scheduler(optimizer, scheduler_params=Config.params):\n",
    "    if scheduler_params['scheduler_name'] == 'CosineAnnealingWarmRestarts':\n",
    "        scheduler = CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=scheduler_params['T_0'],\n",
    "            eta_min=scheduler_params['min_lr'],\n",
    "            last_epoch=-1\n",
    "        )\n",
    "    elif scheduler_params['scheduler_name'] == 'OneCycleLR':\n",
    "        scheduler = OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=scheduler_params['max_lr'],\n",
    "            steps_per_epoch=int(7088 / Config.batch_size) + 1,\n",
    "            epochs=Config.epochs,\n",
    "            pct_start=scheduler_params['pct_start'],\n",
    "            anneal_strategy=scheduler_params['anneal_strategy'],\n",
    "            div_factor=scheduler_params['div_factor'],\n",
    "            final_div_factor=scheduler_params['final_div_factor'],\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v2-xxlarge were not used when initializing DebertaV2Model: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.encoder.layer.0.attention.self.query_proj.weight 2e-06\n",
      "model.encoder.layer.0.attention.self.query_proj.bias 2e-06\n",
      "model.encoder.layer.0.attention.self.key_proj.weight 2e-06\n",
      "model.encoder.layer.0.attention.self.key_proj.bias 2e-06\n",
      "model.encoder.layer.0.attention.self.value_proj.weight 2e-06\n",
      "model.encoder.layer.0.attention.self.value_proj.bias 2e-06\n",
      "model.encoder.layer.0.attention.output.dense.weight 2e-06\n",
      "model.encoder.layer.0.attention.output.dense.bias 2e-06\n",
      "model.encoder.layer.0.attention.output.LayerNorm.weight 2e-06\n",
      "model.encoder.layer.0.attention.output.LayerNorm.bias 2e-06\n",
      "model.encoder.layer.0.intermediate.dense.weight 2e-06\n",
      "model.encoder.layer.0.intermediate.dense.bias 2e-06\n",
      "model.encoder.layer.0.output.dense.weight 2e-06\n",
      "model.encoder.layer.0.output.dense.bias 2e-06\n",
      "model.encoder.layer.0.output.LayerNorm.weight 2e-06\n",
      "model.encoder.layer.0.output.LayerNorm.bias 2e-06\n",
      "model.encoder.layer.1.attention.self.query_proj.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.self.query_proj.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.self.key_proj.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.self.key_proj.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.self.value_proj.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.self.value_proj.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.output.dense.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.output.dense.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.output.LayerNorm.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.attention.output.LayerNorm.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.1.intermediate.dense.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.intermediate.dense.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.1.output.dense.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.output.dense.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.1.output.LayerNorm.weight 2.0425531914893613e-06\n",
      "model.encoder.layer.1.output.LayerNorm.bias 2.0425531914893613e-06\n",
      "model.encoder.layer.2.attention.self.query_proj.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.self.query_proj.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.self.key_proj.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.self.key_proj.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.self.value_proj.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.self.value_proj.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.output.dense.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.output.dense.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.output.LayerNorm.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.attention.output.LayerNorm.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.2.intermediate.dense.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.intermediate.dense.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.2.output.dense.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.output.dense.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.2.output.LayerNorm.weight 2.0851063829787235e-06\n",
      "model.encoder.layer.2.output.LayerNorm.bias 2.0851063829787235e-06\n",
      "model.encoder.layer.3.attention.self.query_proj.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.self.query_proj.bias 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.self.key_proj.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.self.key_proj.bias 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.self.value_proj.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.self.value_proj.bias 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.output.dense.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.output.dense.bias 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.output.LayerNorm.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.attention.output.LayerNorm.bias 2.127659574468085e-06\n",
      "model.encoder.layer.3.intermediate.dense.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.intermediate.dense.bias 2.127659574468085e-06\n",
      "model.encoder.layer.3.output.dense.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.output.dense.bias 2.127659574468085e-06\n",
      "model.encoder.layer.3.output.LayerNorm.weight 2.127659574468085e-06\n",
      "model.encoder.layer.3.output.LayerNorm.bias 2.127659574468085e-06\n",
      "model.encoder.layer.4.attention.self.query_proj.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.self.query_proj.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.self.key_proj.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.self.key_proj.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.self.value_proj.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.self.value_proj.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.output.dense.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.output.dense.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.output.LayerNorm.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.attention.output.LayerNorm.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.4.intermediate.dense.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.intermediate.dense.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.4.output.dense.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.output.dense.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.4.output.LayerNorm.weight 2.1702127659574467e-06\n",
      "model.encoder.layer.4.output.LayerNorm.bias 2.1702127659574467e-06\n",
      "model.encoder.layer.5.attention.self.query_proj.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.self.query_proj.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.self.key_proj.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.self.key_proj.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.self.value_proj.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.self.value_proj.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.output.dense.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.output.dense.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.output.LayerNorm.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.attention.output.LayerNorm.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.5.intermediate.dense.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.intermediate.dense.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.5.output.dense.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.output.dense.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.5.output.LayerNorm.weight 2.2127659574468085e-06\n",
      "model.encoder.layer.5.output.LayerNorm.bias 2.2127659574468085e-06\n",
      "model.encoder.layer.6.attention.self.query_proj.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.self.query_proj.bias 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.self.key_proj.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.self.key_proj.bias 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.self.value_proj.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.self.value_proj.bias 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.output.dense.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.output.dense.bias 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.output.LayerNorm.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.attention.output.LayerNorm.bias 2.25531914893617e-06\n",
      "model.encoder.layer.6.intermediate.dense.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.intermediate.dense.bias 2.25531914893617e-06\n",
      "model.encoder.layer.6.output.dense.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.output.dense.bias 2.25531914893617e-06\n",
      "model.encoder.layer.6.output.LayerNorm.weight 2.25531914893617e-06\n",
      "model.encoder.layer.6.output.LayerNorm.bias 2.25531914893617e-06\n",
      "model.encoder.layer.7.attention.self.query_proj.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.self.query_proj.bias 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.self.key_proj.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.self.key_proj.bias 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.self.value_proj.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.self.value_proj.bias 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.output.dense.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.output.dense.bias 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.output.LayerNorm.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.attention.output.LayerNorm.bias 2.297872340425532e-06\n",
      "model.encoder.layer.7.intermediate.dense.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.intermediate.dense.bias 2.297872340425532e-06\n",
      "model.encoder.layer.7.output.dense.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.output.dense.bias 2.297872340425532e-06\n",
      "model.encoder.layer.7.output.LayerNorm.weight 2.297872340425532e-06\n",
      "model.encoder.layer.7.output.LayerNorm.bias 2.297872340425532e-06\n",
      "model.encoder.layer.8.attention.self.query_proj.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.self.query_proj.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.self.key_proj.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.self.key_proj.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.self.value_proj.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.self.value_proj.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.output.dense.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.output.dense.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.output.LayerNorm.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.attention.output.LayerNorm.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.8.intermediate.dense.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.intermediate.dense.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.8.output.dense.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.output.dense.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.8.output.LayerNorm.weight 2.3404255319148935e-06\n",
      "model.encoder.layer.8.output.LayerNorm.bias 2.3404255319148935e-06\n",
      "model.encoder.layer.9.attention.self.query_proj.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.self.query_proj.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.self.key_proj.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.self.key_proj.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.self.value_proj.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.self.value_proj.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.output.dense.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.output.dense.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.output.LayerNorm.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.attention.output.LayerNorm.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.9.intermediate.dense.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.intermediate.dense.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.9.output.dense.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.output.dense.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.9.output.LayerNorm.weight 2.3829787234042553e-06\n",
      "model.encoder.layer.9.output.LayerNorm.bias 2.3829787234042553e-06\n",
      "model.encoder.layer.10.attention.self.query_proj.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.self.query_proj.bias 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.self.key_proj.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.self.key_proj.bias 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.self.value_proj.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.self.value_proj.bias 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.output.dense.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.output.dense.bias 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.output.LayerNorm.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.attention.output.LayerNorm.bias 2.425531914893617e-06\n",
      "model.encoder.layer.10.intermediate.dense.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.intermediate.dense.bias 2.425531914893617e-06\n",
      "model.encoder.layer.10.output.dense.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.output.dense.bias 2.425531914893617e-06\n",
      "model.encoder.layer.10.output.LayerNorm.weight 2.425531914893617e-06\n",
      "model.encoder.layer.10.output.LayerNorm.bias 2.425531914893617e-06\n",
      "model.encoder.layer.11.attention.self.query_proj.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.self.query_proj.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.self.key_proj.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.self.key_proj.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.self.value_proj.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.self.value_proj.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.output.dense.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.output.dense.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.output.LayerNorm.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.attention.output.LayerNorm.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.11.intermediate.dense.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.intermediate.dense.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.11.output.dense.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.output.dense.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.11.output.LayerNorm.weight 2.4680851063829784e-06\n",
      "model.encoder.layer.11.output.LayerNorm.bias 2.4680851063829784e-06\n",
      "model.encoder.layer.12.attention.self.query_proj.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.self.query_proj.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.self.key_proj.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.self.key_proj.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.self.value_proj.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.self.value_proj.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.output.dense.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.output.dense.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.output.LayerNorm.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.attention.output.LayerNorm.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.12.intermediate.dense.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.intermediate.dense.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.12.output.dense.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.output.dense.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.12.output.LayerNorm.weight 2.5106382978723402e-06\n",
      "model.encoder.layer.12.output.LayerNorm.bias 2.5106382978723402e-06\n",
      "model.encoder.layer.13.attention.self.query_proj.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.self.query_proj.bias 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.self.key_proj.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.self.key_proj.bias 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.self.value_proj.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.self.value_proj.bias 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.output.dense.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.output.dense.bias 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.output.LayerNorm.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.attention.output.LayerNorm.bias 2.553191489361702e-06\n",
      "model.encoder.layer.13.intermediate.dense.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.intermediate.dense.bias 2.553191489361702e-06\n",
      "model.encoder.layer.13.output.dense.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.output.dense.bias 2.553191489361702e-06\n",
      "model.encoder.layer.13.output.LayerNorm.weight 2.553191489361702e-06\n",
      "model.encoder.layer.13.output.LayerNorm.bias 2.553191489361702e-06\n",
      "model.encoder.layer.14.attention.self.query_proj.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.self.query_proj.bias 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.self.key_proj.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.self.key_proj.bias 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.self.value_proj.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.self.value_proj.bias 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.output.dense.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.output.dense.bias 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.output.LayerNorm.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.attention.output.LayerNorm.bias 2.595744680851064e-06\n",
      "model.encoder.layer.14.intermediate.dense.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.intermediate.dense.bias 2.595744680851064e-06\n",
      "model.encoder.layer.14.output.dense.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.output.dense.bias 2.595744680851064e-06\n",
      "model.encoder.layer.14.output.LayerNorm.weight 2.595744680851064e-06\n",
      "model.encoder.layer.14.output.LayerNorm.bias 2.595744680851064e-06\n",
      "model.encoder.layer.15.attention.self.query_proj.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.self.query_proj.bias 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.self.key_proj.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.self.key_proj.bias 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.self.value_proj.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.self.value_proj.bias 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.output.dense.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.output.dense.bias 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.output.LayerNorm.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.attention.output.LayerNorm.bias 2.638297872340425e-06\n",
      "model.encoder.layer.15.intermediate.dense.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.intermediate.dense.bias 2.638297872340425e-06\n",
      "model.encoder.layer.15.output.dense.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.output.dense.bias 2.638297872340425e-06\n",
      "model.encoder.layer.15.output.LayerNorm.weight 2.638297872340425e-06\n",
      "model.encoder.layer.15.output.LayerNorm.bias 2.638297872340425e-06\n",
      "model.encoder.layer.16.attention.self.query_proj.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.self.query_proj.bias 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.self.key_proj.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.self.key_proj.bias 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.self.value_proj.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.self.value_proj.bias 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.output.dense.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.output.dense.bias 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.output.LayerNorm.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.attention.output.LayerNorm.bias 2.680851063829787e-06\n",
      "model.encoder.layer.16.intermediate.dense.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.intermediate.dense.bias 2.680851063829787e-06\n",
      "model.encoder.layer.16.output.dense.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.output.dense.bias 2.680851063829787e-06\n",
      "model.encoder.layer.16.output.LayerNorm.weight 2.680851063829787e-06\n",
      "model.encoder.layer.16.output.LayerNorm.bias 2.680851063829787e-06\n",
      "model.encoder.layer.17.attention.self.query_proj.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.self.query_proj.bias 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.self.key_proj.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.self.key_proj.bias 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.self.value_proj.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.self.value_proj.bias 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.output.dense.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.output.dense.bias 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.output.LayerNorm.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.attention.output.LayerNorm.bias 2.723404255319149e-06\n",
      "model.encoder.layer.17.intermediate.dense.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.intermediate.dense.bias 2.723404255319149e-06\n",
      "model.encoder.layer.17.output.dense.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.output.dense.bias 2.723404255319149e-06\n",
      "model.encoder.layer.17.output.LayerNorm.weight 2.723404255319149e-06\n",
      "model.encoder.layer.17.output.LayerNorm.bias 2.723404255319149e-06\n",
      "model.encoder.layer.18.attention.self.query_proj.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.self.query_proj.bias 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.self.key_proj.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.self.key_proj.bias 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.self.value_proj.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.self.value_proj.bias 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.output.dense.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.output.dense.bias 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.output.LayerNorm.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.attention.output.LayerNorm.bias 2.76595744680851e-06\n",
      "model.encoder.layer.18.intermediate.dense.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.intermediate.dense.bias 2.76595744680851e-06\n",
      "model.encoder.layer.18.output.dense.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.output.dense.bias 2.76595744680851e-06\n",
      "model.encoder.layer.18.output.LayerNorm.weight 2.76595744680851e-06\n",
      "model.encoder.layer.18.output.LayerNorm.bias 2.76595744680851e-06\n",
      "model.encoder.layer.19.attention.self.query_proj.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.self.query_proj.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.self.key_proj.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.self.key_proj.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.self.value_proj.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.self.value_proj.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.output.dense.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.output.dense.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.output.LayerNorm.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.attention.output.LayerNorm.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.19.intermediate.dense.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.intermediate.dense.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.19.output.dense.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.output.dense.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.19.output.LayerNorm.weight 2.8085106382978724e-06\n",
      "model.encoder.layer.19.output.LayerNorm.bias 2.8085106382978724e-06\n",
      "model.encoder.layer.20.attention.self.query_proj.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.self.query_proj.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.self.key_proj.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.self.key_proj.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.self.value_proj.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.self.value_proj.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.output.dense.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.output.dense.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.output.LayerNorm.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.attention.output.LayerNorm.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.20.intermediate.dense.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.intermediate.dense.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.20.output.dense.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.output.dense.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.20.output.LayerNorm.weight 2.8510638297872338e-06\n",
      "model.encoder.layer.20.output.LayerNorm.bias 2.8510638297872338e-06\n",
      "model.encoder.layer.21.attention.self.query_proj.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.self.query_proj.bias 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.self.key_proj.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.self.key_proj.bias 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.self.value_proj.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.self.value_proj.bias 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.output.dense.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.output.dense.bias 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.output.LayerNorm.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.attention.output.LayerNorm.bias 2.893617021276595e-06\n",
      "model.encoder.layer.21.intermediate.dense.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.intermediate.dense.bias 2.893617021276595e-06\n",
      "model.encoder.layer.21.output.dense.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.output.dense.bias 2.893617021276595e-06\n",
      "model.encoder.layer.21.output.LayerNorm.weight 2.893617021276595e-06\n",
      "model.encoder.layer.21.output.LayerNorm.bias 2.893617021276595e-06\n",
      "model.encoder.layer.22.attention.self.query_proj.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.self.query_proj.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.self.key_proj.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.self.key_proj.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.self.value_proj.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.self.value_proj.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.output.dense.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.output.dense.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.output.LayerNorm.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.attention.output.LayerNorm.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.22.intermediate.dense.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.intermediate.dense.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.22.output.dense.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.output.dense.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.22.output.LayerNorm.weight 2.9361702127659574e-06\n",
      "model.encoder.layer.22.output.LayerNorm.bias 2.9361702127659574e-06\n",
      "model.encoder.layer.23.attention.self.query_proj.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.self.query_proj.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.self.key_proj.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.self.key_proj.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.self.value_proj.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.self.value_proj.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.output.dense.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.output.dense.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.output.LayerNorm.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.attention.output.LayerNorm.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.23.intermediate.dense.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.intermediate.dense.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.23.output.dense.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.output.dense.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.23.output.LayerNorm.weight 2.9787234042553188e-06\n",
      "model.encoder.layer.23.output.LayerNorm.bias 2.9787234042553188e-06\n",
      "model.encoder.layer.24.attention.self.query_proj.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.self.query_proj.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.self.key_proj.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.self.key_proj.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.self.value_proj.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.self.value_proj.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.output.dense.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.output.dense.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.output.LayerNorm.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.attention.output.LayerNorm.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.24.intermediate.dense.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.intermediate.dense.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.24.output.dense.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.output.dense.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.24.output.LayerNorm.weight 3.0212765957446806e-06\n",
      "model.encoder.layer.24.output.LayerNorm.bias 3.0212765957446806e-06\n",
      "model.encoder.layer.25.attention.self.query_proj.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.self.query_proj.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.self.key_proj.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.self.key_proj.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.self.value_proj.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.self.value_proj.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.output.dense.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.output.dense.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.output.LayerNorm.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.attention.output.LayerNorm.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.25.intermediate.dense.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.intermediate.dense.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.25.output.dense.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.output.dense.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.25.output.LayerNorm.weight 3.0638297872340424e-06\n",
      "model.encoder.layer.25.output.LayerNorm.bias 3.0638297872340424e-06\n",
      "model.encoder.layer.26.attention.self.query_proj.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.self.query_proj.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.self.key_proj.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.self.key_proj.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.self.value_proj.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.self.value_proj.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.output.dense.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.output.dense.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.output.LayerNorm.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.attention.output.LayerNorm.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.26.intermediate.dense.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.intermediate.dense.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.26.output.dense.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.output.dense.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.26.output.LayerNorm.weight 3.1063829787234046e-06\n",
      "model.encoder.layer.26.output.LayerNorm.bias 3.1063829787234046e-06\n",
      "model.encoder.layer.27.attention.self.query_proj.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.self.query_proj.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.self.key_proj.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.self.key_proj.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.self.value_proj.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.self.value_proj.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.output.dense.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.output.dense.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.output.LayerNorm.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.attention.output.LayerNorm.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.27.intermediate.dense.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.intermediate.dense.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.27.output.dense.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.output.dense.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.27.output.LayerNorm.weight 3.1489361702127655e-06\n",
      "model.encoder.layer.27.output.LayerNorm.bias 3.1489361702127655e-06\n",
      "model.encoder.layer.28.attention.self.query_proj.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.self.query_proj.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.self.key_proj.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.self.key_proj.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.self.value_proj.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.self.value_proj.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.output.dense.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.output.dense.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.output.LayerNorm.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.attention.output.LayerNorm.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.28.intermediate.dense.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.intermediate.dense.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.28.output.dense.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.output.dense.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.28.output.LayerNorm.weight 3.1914893617021273e-06\n",
      "model.encoder.layer.28.output.LayerNorm.bias 3.1914893617021273e-06\n",
      "model.encoder.layer.29.attention.self.query_proj.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.self.query_proj.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.self.key_proj.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.self.key_proj.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.self.value_proj.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.self.value_proj.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.output.dense.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.output.dense.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.output.LayerNorm.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.attention.output.LayerNorm.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.29.intermediate.dense.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.intermediate.dense.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.29.output.dense.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.output.dense.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.29.output.LayerNorm.weight 3.2340425531914895e-06\n",
      "model.encoder.layer.29.output.LayerNorm.bias 3.2340425531914895e-06\n",
      "model.encoder.layer.30.attention.self.query_proj.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.self.query_proj.bias 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.self.key_proj.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.self.key_proj.bias 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.self.value_proj.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.self.value_proj.bias 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.output.dense.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.output.dense.bias 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.output.LayerNorm.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.attention.output.LayerNorm.bias 3.276595744680851e-06\n",
      "model.encoder.layer.30.intermediate.dense.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.intermediate.dense.bias 3.276595744680851e-06\n",
      "model.encoder.layer.30.output.dense.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.output.dense.bias 3.276595744680851e-06\n",
      "model.encoder.layer.30.output.LayerNorm.weight 3.276595744680851e-06\n",
      "model.encoder.layer.30.output.LayerNorm.bias 3.276595744680851e-06\n",
      "model.encoder.layer.31.attention.self.query_proj.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.self.query_proj.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.self.key_proj.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.self.key_proj.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.self.value_proj.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.self.value_proj.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.output.dense.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.output.dense.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.output.LayerNorm.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.attention.output.LayerNorm.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.31.intermediate.dense.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.intermediate.dense.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.31.output.dense.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.output.dense.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.31.output.LayerNorm.weight 3.3191489361702127e-06\n",
      "model.encoder.layer.31.output.LayerNorm.bias 3.3191489361702127e-06\n",
      "model.encoder.layer.32.attention.self.query_proj.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.self.query_proj.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.self.key_proj.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.self.key_proj.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.self.value_proj.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.self.value_proj.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.output.dense.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.output.dense.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.output.LayerNorm.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.attention.output.LayerNorm.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.32.intermediate.dense.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.intermediate.dense.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.32.output.dense.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.output.dense.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.32.output.LayerNorm.weight 3.3617021276595745e-06\n",
      "model.encoder.layer.32.output.LayerNorm.bias 3.3617021276595745e-06\n",
      "model.encoder.layer.33.attention.self.query_proj.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.self.query_proj.bias 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.self.key_proj.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.self.key_proj.bias 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.self.value_proj.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.self.value_proj.bias 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.output.dense.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.output.dense.bias 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.output.LayerNorm.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.attention.output.LayerNorm.bias 3.404255319148936e-06\n",
      "model.encoder.layer.33.intermediate.dense.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.intermediate.dense.bias 3.404255319148936e-06\n",
      "model.encoder.layer.33.output.dense.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.output.dense.bias 3.404255319148936e-06\n",
      "model.encoder.layer.33.output.LayerNorm.weight 3.404255319148936e-06\n",
      "model.encoder.layer.33.output.LayerNorm.bias 3.404255319148936e-06\n",
      "model.encoder.layer.34.attention.self.query_proj.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.self.query_proj.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.self.key_proj.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.self.key_proj.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.self.value_proj.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.self.value_proj.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.output.dense.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.output.dense.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.output.LayerNorm.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.attention.output.LayerNorm.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.34.intermediate.dense.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.intermediate.dense.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.34.output.dense.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.output.dense.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.34.output.LayerNorm.weight 3.4468085106382977e-06\n",
      "model.encoder.layer.34.output.LayerNorm.bias 3.4468085106382977e-06\n",
      "model.encoder.layer.35.attention.self.query_proj.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.self.query_proj.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.self.key_proj.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.self.key_proj.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.self.value_proj.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.self.value_proj.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.output.dense.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.output.dense.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.output.LayerNorm.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.attention.output.LayerNorm.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.35.intermediate.dense.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.intermediate.dense.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.35.output.dense.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.output.dense.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.35.output.LayerNorm.weight 3.4893617021276595e-06\n",
      "model.encoder.layer.35.output.LayerNorm.bias 3.4893617021276595e-06\n",
      "model.encoder.layer.36.attention.self.query_proj.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.self.query_proj.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.self.key_proj.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.self.key_proj.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.self.value_proj.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.self.value_proj.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.output.dense.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.output.dense.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.output.LayerNorm.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.attention.output.LayerNorm.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.36.intermediate.dense.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.intermediate.dense.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.36.output.dense.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.output.dense.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.36.output.LayerNorm.weight 3.5319148936170213e-06\n",
      "model.encoder.layer.36.output.LayerNorm.bias 3.5319148936170213e-06\n",
      "model.encoder.layer.37.attention.self.query_proj.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.self.query_proj.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.self.key_proj.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.self.key_proj.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.self.value_proj.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.self.value_proj.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.output.dense.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.output.dense.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.output.LayerNorm.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.attention.output.LayerNorm.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.37.intermediate.dense.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.intermediate.dense.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.37.output.dense.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.output.dense.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.37.output.LayerNorm.weight 3.5744680851063827e-06\n",
      "model.encoder.layer.37.output.LayerNorm.bias 3.5744680851063827e-06\n",
      "model.encoder.layer.38.attention.self.query_proj.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.self.query_proj.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.self.key_proj.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.self.key_proj.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.self.value_proj.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.self.value_proj.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.output.dense.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.output.dense.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.output.LayerNorm.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.attention.output.LayerNorm.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.38.intermediate.dense.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.intermediate.dense.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.38.output.dense.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.output.dense.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.38.output.LayerNorm.weight 3.6170212765957445e-06\n",
      "model.encoder.layer.38.output.LayerNorm.bias 3.6170212765957445e-06\n",
      "model.encoder.layer.39.attention.self.query_proj.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.self.query_proj.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.self.key_proj.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.self.key_proj.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.self.value_proj.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.self.value_proj.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.output.dense.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.output.dense.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.output.LayerNorm.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.attention.output.LayerNorm.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.39.intermediate.dense.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.intermediate.dense.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.39.output.dense.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.output.dense.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.39.output.LayerNorm.weight 3.6595744680851063e-06\n",
      "model.encoder.layer.39.output.LayerNorm.bias 3.6595744680851063e-06\n",
      "model.encoder.layer.40.attention.self.query_proj.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.self.query_proj.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.self.key_proj.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.self.key_proj.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.self.value_proj.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.self.value_proj.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.output.dense.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.output.dense.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.output.LayerNorm.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.attention.output.LayerNorm.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.40.intermediate.dense.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.intermediate.dense.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.40.output.dense.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.output.dense.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.40.output.LayerNorm.weight 3.7021276595744676e-06\n",
      "model.encoder.layer.40.output.LayerNorm.bias 3.7021276595744676e-06\n",
      "model.encoder.layer.41.attention.self.query_proj.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.self.query_proj.bias 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.self.key_proj.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.self.key_proj.bias 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.self.value_proj.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.self.value_proj.bias 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.output.dense.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.output.dense.bias 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.output.LayerNorm.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.attention.output.LayerNorm.bias 3.74468085106383e-06\n",
      "model.encoder.layer.41.intermediate.dense.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.intermediate.dense.bias 3.74468085106383e-06\n",
      "model.encoder.layer.41.output.dense.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.output.dense.bias 3.74468085106383e-06\n",
      "model.encoder.layer.41.output.LayerNorm.weight 3.74468085106383e-06\n",
      "model.encoder.layer.41.output.LayerNorm.bias 3.74468085106383e-06\n",
      "model.encoder.layer.42.attention.self.query_proj.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.self.query_proj.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.self.key_proj.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.self.key_proj.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.self.value_proj.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.self.value_proj.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.output.dense.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.output.dense.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.output.LayerNorm.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.attention.output.LayerNorm.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.42.intermediate.dense.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.intermediate.dense.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.42.output.dense.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.output.dense.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.42.output.LayerNorm.weight 3.7872340425531912e-06\n",
      "model.encoder.layer.42.output.LayerNorm.bias 3.7872340425531912e-06\n",
      "model.encoder.layer.43.attention.self.query_proj.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.self.query_proj.bias 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.self.key_proj.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.self.key_proj.bias 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.self.value_proj.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.self.value_proj.bias 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.output.dense.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.output.dense.bias 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.output.LayerNorm.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.attention.output.LayerNorm.bias 3.829787234042553e-06\n",
      "model.encoder.layer.43.intermediate.dense.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.intermediate.dense.bias 3.829787234042553e-06\n",
      "model.encoder.layer.43.output.dense.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.output.dense.bias 3.829787234042553e-06\n",
      "model.encoder.layer.43.output.LayerNorm.weight 3.829787234042553e-06\n",
      "model.encoder.layer.43.output.LayerNorm.bias 3.829787234042553e-06\n",
      "model.encoder.layer.44.attention.self.query_proj.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.self.query_proj.bias 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.self.key_proj.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.self.key_proj.bias 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.self.value_proj.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.self.value_proj.bias 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.output.dense.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.output.dense.bias 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.output.LayerNorm.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.attention.output.LayerNorm.bias 3.872340425531914e-06\n",
      "model.encoder.layer.44.intermediate.dense.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.intermediate.dense.bias 3.872340425531914e-06\n",
      "model.encoder.layer.44.output.dense.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.output.dense.bias 3.872340425531914e-06\n",
      "model.encoder.layer.44.output.LayerNorm.weight 3.872340425531914e-06\n",
      "model.encoder.layer.44.output.LayerNorm.bias 3.872340425531914e-06\n",
      "model.encoder.layer.45.attention.self.query_proj.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.self.query_proj.bias 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.self.key_proj.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.self.key_proj.bias 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.self.value_proj.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.self.value_proj.bias 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.output.dense.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.output.dense.bias 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.output.LayerNorm.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.attention.output.LayerNorm.bias 3.914893617021276e-06\n",
      "model.encoder.layer.45.intermediate.dense.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.intermediate.dense.bias 3.914893617021276e-06\n",
      "model.encoder.layer.45.output.dense.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.output.dense.bias 3.914893617021276e-06\n",
      "model.encoder.layer.45.output.LayerNorm.weight 3.914893617021276e-06\n",
      "model.encoder.layer.45.output.LayerNorm.bias 3.914893617021276e-06\n",
      "model.encoder.layer.46.attention.self.query_proj.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.self.query_proj.bias 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.self.key_proj.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.self.key_proj.bias 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.self.value_proj.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.self.value_proj.bias 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.output.dense.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.output.dense.bias 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.output.LayerNorm.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.attention.output.LayerNorm.bias 3.957446808510638e-06\n",
      "model.encoder.layer.46.intermediate.dense.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.intermediate.dense.bias 3.957446808510638e-06\n",
      "model.encoder.layer.46.output.dense.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.output.dense.bias 3.957446808510638e-06\n",
      "model.encoder.layer.46.output.LayerNorm.weight 3.957446808510638e-06\n",
      "model.encoder.layer.46.output.LayerNorm.bias 3.957446808510638e-06\n",
      "model.encoder.layer.47.attention.self.query_proj.weight 4e-06\n",
      "model.encoder.layer.47.attention.self.query_proj.bias 4e-06\n",
      "model.encoder.layer.47.attention.self.key_proj.weight 4e-06\n",
      "model.encoder.layer.47.attention.self.key_proj.bias 4e-06\n",
      "model.encoder.layer.47.attention.self.value_proj.weight 4e-06\n",
      "model.encoder.layer.47.attention.self.value_proj.bias 4e-06\n",
      "model.encoder.layer.47.attention.output.dense.weight 4e-06\n",
      "model.encoder.layer.47.attention.output.dense.bias 4e-06\n",
      "model.encoder.layer.47.attention.output.LayerNorm.weight 4e-06\n",
      "model.encoder.layer.47.attention.output.LayerNorm.bias 4e-06\n",
      "model.encoder.layer.47.intermediate.dense.weight 4e-06\n",
      "model.encoder.layer.47.intermediate.dense.bias 4e-06\n",
      "model.encoder.layer.47.output.dense.weight 4e-06\n",
      "model.encoder.layer.47.output.dense.bias 4e-06\n",
      "model.encoder.layer.47.output.LayerNorm.weight 4e-06\n",
      "model.encoder.layer.47.output.LayerNorm.bias 4e-06\n",
      "conv3.lin_rel.weight 4e-06\n",
      "conv3.lin_rel.bias 4e-06\n",
      "conv3.lin_root.weight 4e-06\n",
      "conv1.weight 4e-06\n",
      "conv1.root 4e-06\n",
      "conv1.bias 4e-06\n",
      "encoder_layer1.self_attn.in_proj_weight 4e-06\n",
      "encoder_layer1.self_attn.in_proj_bias 4e-06\n",
      "encoder_layer1.self_attn.out_proj.weight 4e-06\n",
      "encoder_layer1.self_attn.out_proj.bias 4e-06\n",
      "encoder_layer1.linear1.weight 4e-06\n",
      "encoder_layer1.linear1.bias 4e-06\n",
      "encoder_layer1.linear2.weight 4e-06\n",
      "encoder_layer1.linear2.bias 4e-06\n",
      "encoder_layer1.norm1.weight 4e-06\n",
      "encoder_layer1.norm1.bias 4e-06\n",
      "encoder_layer1.norm2.weight 4e-06\n",
      "encoder_layer1.norm2.bias 4e-06\n",
      "encoder_layer2.self_attn.in_proj_weight 4e-06\n",
      "encoder_layer2.self_attn.in_proj_bias 4e-06\n",
      "encoder_layer2.self_attn.out_proj.weight 4e-06\n",
      "encoder_layer2.self_attn.out_proj.bias 4e-06\n",
      "encoder_layer2.linear1.weight 4e-06\n",
      "encoder_layer2.linear1.bias 4e-06\n",
      "encoder_layer2.linear2.weight 4e-06\n",
      "encoder_layer2.linear2.bias 4e-06\n",
      "encoder_layer2.norm1.weight 4e-06\n",
      "encoder_layer2.norm1.bias 4e-06\n",
      "encoder_layer2.norm2.weight 4e-06\n",
      "encoder_layer2.norm2.bias 4e-06\n",
      "linear1.weight 4e-06\n",
      "linear1.bias 4e-06\n",
      "linear2.weight 4e-06\n",
      "linear2.bias 4e-06\n",
      "coattention.q1.weight 4e-06\n",
      "coattention.q1.bias 4e-06\n",
      "coattention.q2.weight 4e-06\n",
      "coattention.q2.bias 4e-06\n",
      "coattention.q3.weight 4e-06\n",
      "coattention.q3.bias 4e-06\n",
      "coattention.q4.weight 4e-06\n",
      "coattention.q4.bias 4e-06\n",
      "coattention.k1.weight 4e-06\n",
      "coattention.k1.bias 4e-06\n",
      "coattention.k2.weight 4e-06\n",
      "coattention.k2.bias 4e-06\n",
      "coattention.k3.weight 4e-06\n",
      "coattention.k3.bias 4e-06\n",
      "coattention.k4.weight 4e-06\n",
      "coattention.k4.bias 4e-06\n",
      "coattention.v1.weight 4e-06\n",
      "coattention.v1.bias 4e-06\n",
      "coattention.v2.weight 4e-06\n",
      "coattention.v2.bias 4e-06\n",
      "coattention.v3.weight 4e-06\n",
      "coattention.v3.bias 4e-06\n",
      "coattention.v4.weight 4e-06\n",
      "coattention.v4.bias 4e-06\n",
      "coattention.layer_norm.weight 4e-06\n",
      "coattention.layer_norm.bias 4e-06\n",
      "linear4.weight 4e-06\n",
      "linear4.bias 4e-06\n",
      "linear5.weight 4e-06\n",
      "linear5.bias 4e-06\n",
      "linear6.weight 4e-06\n",
      "linear6.bias 4e-06\n",
      "******************** Training Epoch: 1 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 01. Train. Loss: 1.3363: 100%|██████████| 3544/3544 [37:24<00:00,  1.58it/s]\n",
      "Epoch: 01. Valid. R4_1: 0.5305 R4_2: 0.7833 MRR: 0.7212: 100%|██████████| 443/443 [01:44<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5304740406320542 model saved\n",
      "******************** Training Epoch: 2 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 02. Train. Loss: 0.7064: 100%|██████████| 3544/3544 [37:16<00:00,  1.58it/s]\n",
      "Epoch: 02. Valid. R4_1: 0.8160 R4_2: 0.9470 MRR: 0.8979: 100%|██████████| 443/443 [01:44<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8160270880361173 model saved\n",
      "******************** Training Epoch: 3 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 03. Train. Loss: 0.4768: 100%|██████████| 3544/3544 [37:16<00:00,  1.58it/s]\n",
      "Epoch: 03. Valid. R4_1: 0.8307 R4_2: 0.9537 MRR: 0.9065: 100%|██████████| 443/443 [01:44<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8306997742663657 model saved\n",
      "******************** Training Epoch: 4 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 04. Train. Loss: 0.3685: 100%|██████████| 3544/3544 [37:19<00:00,  1.58it/s]\n",
      "Epoch: 04. Valid. R4_1: 0.8330 R4_2: 0.9526 MRR: 0.9078: 100%|██████████| 443/443 [01:44<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8329571106094809 model saved\n",
      "******************** Training Epoch: 5 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 05. Train. Loss: 0.3004: 100%|██████████| 3544/3544 [37:23<00:00,  1.58it/s]\n",
      "Epoch: 06. Train. Loss: 0.2740: 100%|██████████| 3544/3544 [37:08<00:00,  1.59it/s]01:21<00:21,  4.10it/s]\n",
      "Epoch: 06. Valid. R4_1: 0.8488 R4_2: 0.9526 MRR: 0.9154: 100%|██████████| 443/443 [01:43<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8487584650112867 model saved\n",
      "******************** Training Epoch: 7 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 07. Train. Loss: 0.2496: 100%|██████████| 3544/3544 [37:15<00:00,  1.59it/s]\n",
      "Epoch: 07. Valid. R4_1: 0.8499 R4_2: 0.9515 MRR: 0.9160: 100%|██████████| 443/443 [01:44<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8498871331828443 model saved\n",
      "******************** Training Epoch: 8 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 08. Train. Loss: 0.2358: 100%|██████████| 3544/3544 [37:22<00:00,  1.58it/s]\n",
      "Epoch: 08. Valid. R4_1: 0.8634 R4_2: 0.9582 MRR: 0.9240: 100%|██████████| 443/443 [01:44<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.863431151241535 model saved\n",
      "******************** Training Epoch: 9 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 09. Train. Loss: 0.2312: 100%|██████████| 3544/3544 [37:21<00:00,  1.58it/s]\n",
      "Epoch: 09. Valid. R4_1: 0.8612 R4_2: 0.9582 MRR: 0.9232: 100%|██████████| 443/443 [01:44<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Training Epoch: 10 ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 10. Train. Loss: 0.2301: 100%|██████████| 3544/3544 [37:18<00:00,  1.58it/s]\n",
      "Epoch: 10. Valid. R4_1: 0.8646 R4_2: 0.9571 MRR: 0.9247: 100%|██████████| 443/443 [01:43<00:00,  4.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8645598194130926 model saved\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "train_dataset = MuTualDataset(train_features)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_dataset = MuTualDataset(val_features)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "model = MuTualModel()\n",
    "\n",
    "#model.load_state_dict(torch.load('autodl-tmp/plus/8352model.bin', map_location=torch.device(device)))\n",
    "model = model.to(device)\n",
    "# param_optimizer = list(model.named_parameters())\n",
    "# no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n",
    "# optimizer_grouped_parameters = [\n",
    "#     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "#     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# ]\n",
    "def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "    named_parameters = list(model.named_parameters())    \n",
    "    parameters = []\n",
    "\n",
    "    # increase lr every second layer\n",
    "    increase_lr_every_k_layer = 1\n",
    "    lrs = np.linspace(1, 2, 48 // increase_lr_every_k_layer)\n",
    "    num = 0\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    for layer_num, (name, params) in enumerate(named_parameters):\n",
    "        weight_decay = 0.0 if any(nd in name for nd in no_decay) else 0.01\n",
    "        splitted_name = name.split('.')\n",
    "        lr = encoder_lr\n",
    "        if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n",
    "            layer_num = int(splitted_name[3])\n",
    "            lr = lrs[layer_num // increase_lr_every_k_layer] * encoder_lr\n",
    "            # num+=1\n",
    "            print(name,lr)\n",
    "        if 'model' not in splitted_name:\n",
    "            lr = lrs[-1]*encoder_lr\n",
    "            print(name,lr)\n",
    "#         if splitted_name[0] in ['fc']:\n",
    "#             lr = 10*encoder_lr\n",
    "#             print(name,lr)\n",
    "\n",
    "#         if splitted_name[0] in ['head']:\n",
    "#             lr = 10*encoder_lr\n",
    "#             print(name,lr)\n",
    "        # print(num)\n",
    "        parameters.append({\"params\": params,\n",
    "                           \"weight_decay\": weight_decay,\n",
    "                           \"lr\": lr})\n",
    "    return parameters\n",
    "# def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_parameters = [\n",
    "#         {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#          'lr': encoder_lr, 'weight_decay': weight_decay, 'initial_lr':encoder_lr},\n",
    "#         {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#          'lr': encoder_lr, 'weight_decay': 0.0, 'initial_lr':encoder_lr},\n",
    "#         {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "#          'lr': decoder_lr, 'weight_decay': 0.0, 'initial_lr':decoder_lr}\n",
    "#     ]\n",
    "#     return optimizer_parameters\n",
    "optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=Config.lr, \n",
    "                                                decoder_lr=Config.lr,\n",
    "                                                weight_decay=0.01)\n",
    "optimizer = AdamW(optimizer_parameters)\n",
    "# optimizer = optim.AdamW(optimizer_grouped_parameters, lr=Config.lr)\n",
    "scheduler = get_scheduler(optimizer)\n",
    "best=0.0\n",
    "for epoch in range(1, Config.epochs + 1):#Config.epochs\n",
    "  print(f'******************** Training Epoch: {epoch} ********************')\n",
    "  train_fn(train_dataloader, model, criterion1, optimizer,scheduler)\n",
    "  r4_1,r4_2,mrr = validate_fn(val_dataloader, model, criterion2)\n",
    "  if r4_1 >= best:\n",
    "        best = r4_1\n",
    "        print(f'{r4_1} model saved')\n",
    "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect()\n",
    "# train_dataset = MuTualDataset(train_features)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# val_dataset = MuTualDataset(val_features)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "# model = MuTualModel()\n",
    "# model = model.to(device)\n",
    "# #model.load_state_dict(torch.load('autodl-tmp/train/9020model.bin', map_location=torch.device(device)))\n",
    "# # param_optimizer = list(model.named_parameters())\n",
    "# # no_decay = ['bias', 'LayerNorm.weight', 'LayerNorm.bias']\n",
    "# # optimizer_grouped_parameters = [\n",
    "# #     {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "# #     {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "# # ]\n",
    "# # def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "# #     named_parameters = list(model.named_parameters())    \n",
    "# #     parameters = []\n",
    "\n",
    "# #     # increase lr every second layer\n",
    "# #     increase_lr_every_k_layer = 1\n",
    "# #     lrs = np.linspace(1, 5, 48 // increase_lr_every_k_layer)\n",
    "# #     num = 0\n",
    "# #     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "# #     for layer_num, (name, params) in enumerate(named_parameters):\n",
    "# #         weight_decay = 0.0 if any(nd in name for nd in no_decay) else 0.01\n",
    "# #         splitted_name = name.split('.')\n",
    "# #         lr = encoder_lr\n",
    "# #         if len(splitted_name) >= 4 and str.isdigit(splitted_name[3]):\n",
    "# #             layer_num = int(splitted_name[3])\n",
    "# #             lr = lrs[layer_num // increase_lr_every_k_layer] * encoder_lr\n",
    "# #             # num+=1\n",
    "# #             print(name,lr)\n",
    "# #         if 'model' not in splitted_name:\n",
    "# #             lr = lrs[-1]*encoder_lr\n",
    "# #             print(name,lr)\n",
    "# # #         if splitted_name[0] in ['fc']:\n",
    "# # #             lr = 10*encoder_lr\n",
    "# # #             print(name,lr)\n",
    "\n",
    "# # #         if splitted_name[0] in ['head']:\n",
    "# # #             lr = 10*encoder_lr\n",
    "# # #             print(name,lr)\n",
    "# #         # print(num)\n",
    "# #         parameters.append({\"params\": params,\n",
    "# #                            \"weight_decay\": weight_decay,\n",
    "# #                            \"lr\": lr})\n",
    "# #     return parameters\n",
    "# def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "#     param_optimizer = list(model.named_parameters())\n",
    "#     no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_parameters = [\n",
    "#         {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "#          'lr': encoder_lr, 'weight_decay': weight_decay, 'initial_lr':encoder_lr},\n",
    "#         {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "#          'lr': encoder_lr, 'weight_decay': 0.0, 'initial_lr':encoder_lr},\n",
    "#         {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "#          'lr': decoder_lr, 'weight_decay': 0.0, 'initial_lr':decoder_lr}\n",
    "#     ]\n",
    "#     return optimizer_parameters\n",
    "# optimizer_parameters = get_optimizer_params(model,\n",
    "#                                                 encoder_lr=Config.lr, \n",
    "#                                                 decoder_lr=Config.lr,\n",
    "#                                                 weight_decay=0.01)\n",
    "# optimizer = AdamW(optimizer_parameters)\n",
    "# # optimizer = optim.AdamW(optimizer_grouped_parameters, lr=Config.lr)\n",
    "# scheduler = get_scheduler(optimizer)\n",
    "# best=0.0\n",
    "# for epoch in range(1, Config.epochs + 1):\n",
    "#   print(f'******************** Training Epoch: {epoch} ********************')\n",
    "#   train_fn(train_dataloader, model, criterion1, optimizer,scheduler)\n",
    "#   r4_1,r4_2,mrr = validate_fn(val_dataloader, model, criterion2)\n",
    "#   if r4_1 >= best:\n",
    "#         best = r4_1\n",
    "#         print(f'{r4_1} model saved')\n",
    "#         torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, f\"model.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "AIy7_jgH1l9n"
   },
   "outputs": [],
   "source": [
    "#model = MuTualModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "BZBxTk6C2Abz"
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"/content/drive/MyDrive/Colab/MuTual/outputbert-base-gat_50_epoch.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "KKm4Ccgj4jp7"
   },
   "outputs": [],
   "source": [
    "#model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "shWXyxYp2Jc-"
   },
   "outputs": [],
   "source": [
    "#validate_fn(val_dataloader, model, criterion2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0c00484cabf84e2eb36826f4b7d1aa6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13e8be12aaad4cd392f76642a5ff7c7a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "144981735bd14cbf86592f9842a8c0f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e6c446a710b4ded9aa3dac0c836d295": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f2a9197594c4a8890cd2485958e80f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a824b6693c4422da00e6a64a51a012b",
      "placeholder": "​",
      "style": "IPY_MODEL_d34174ad7a414212a62a84a386ab0f70",
      "value": " 28.0/28.0 [00:00&lt;00:00, 1.06kB/s]"
     }
    },
    "2f63c480d1574a69ad03d7d4a1d7417e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a824b6693c4422da00e6a64a51a012b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3da2472043fb403a8baca25a84c3fbfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f63c480d1574a69ad03d7d4a1d7417e",
      "placeholder": "​",
      "style": "IPY_MODEL_a2f96386fe184f38ad74700130141290",
      "value": " 455k/455k [00:00&lt;00:00, 4.31MB/s]"
     }
    },
    "3dcb2744e11246bdb409c03ef6929a08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3e5185c08d4c4709a293ec01fa2d598d",
       "IPY_MODEL_8b46d71efaac449d9f1b8eaea7d7437f",
       "IPY_MODEL_3da2472043fb403a8baca25a84c3fbfc"
      ],
      "layout": "IPY_MODEL_d4d620025a2149cf84154718f0f3a889"
     }
    },
    "3e5185c08d4c4709a293ec01fa2d598d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e6c446a710b4ded9aa3dac0c836d295",
      "placeholder": "​",
      "style": "IPY_MODEL_de70481d4882465ebe6f313940b9a093",
      "value": "Downloading: 100%"
     }
    },
    "3f466f3bbb684977a6b4fdf47930b8f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40d5832f202b44c89d8dd37c676b52bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df818c9657b343dc8cd3aa7a1a23e991",
       "IPY_MODEL_fa1784d71c9c48ad844309aceda59def",
       "IPY_MODEL_2f2a9197594c4a8890cd2485958e80f6"
      ],
      "layout": "IPY_MODEL_ee22a71cb96549b09ec6296ef758fb08"
     }
    },
    "42663a30a68043f2978fcfe3712b84fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "456830934621486ea616ac2727081774": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "48bf8cc47df04c128bf39221d8dbd69a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4f7745e2b98542d88c0003641e02ceb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "53384bab4052401a93010f763b6f5c64": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5521f879c1ad42dda99c46c152845629": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_89f84f6a2dd946e6ac0cb164f223d065",
       "IPY_MODEL_944f758a16164c4f87d551f4fe85755b",
       "IPY_MODEL_ccb325512a7344dab2328cd4267dd1c1"
      ],
      "layout": "IPY_MODEL_91337ecf49244217ba63ac92da707e97"
     }
    },
    "62bf451428374cc19490b1c486e9b7a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "69beae0cf0c0440783d5d5fdd8f41bfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6cc45c3a26b84c77ae8dc1bc84a7079a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99f91ba2a8f54adbbad4a1b61e94d80c",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62bf451428374cc19490b1c486e9b7a9",
      "value": 440473133
     }
    },
    "716d7f32b2f143108e4cfc6aff8d3f9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89f84f6a2dd946e6ac0cb164f223d065": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8e8bbe367b4343a2afc2296821e9fe73",
      "placeholder": "​",
      "style": "IPY_MODEL_8b904cba9ef4498f97c9cadfdcb9801f",
      "value": "Downloading: 100%"
     }
    },
    "8b46d71efaac449d9f1b8eaea7d7437f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a475b7c855bf45b48dad5cfd7f8814a7",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4f7745e2b98542d88c0003641e02ceb3",
      "value": 466062
     }
    },
    "8b904cba9ef4498f97c9cadfdcb9801f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8e8bbe367b4343a2afc2296821e9fe73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f1462b0b13c436abdc3d65b77bd7d21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b68255e8b7164376b8444cb197862910",
      "placeholder": "​",
      "style": "IPY_MODEL_e0753dfa37964692b61cfa3a4fe9f697",
      "value": " 420M/420M [00:07&lt;00:00, 60.4MB/s]"
     }
    },
    "9123e6db17fb418280d4390c2b397ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13e8be12aaad4cd392f76642a5ff7c7a",
      "placeholder": "​",
      "style": "IPY_MODEL_456830934621486ea616ac2727081774",
      "value": "Downloading: 100%"
     }
    },
    "91337ecf49244217ba63ac92da707e97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "944f758a16164c4f87d551f4fe85755b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c9001aebde3d4663b3b29a7bddef3ae5",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69beae0cf0c0440783d5d5fdd8f41bfd",
      "value": 231508
     }
    },
    "99cfd3bb5d534331a223d5faa93b244f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99f91ba2a8f54adbbad4a1b61e94d80c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e7bb7cf4f694ab9aaf1d1b8e290f368": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2c4ea5c01a64723ab2f415b2bb8ba57": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f466f3bbb684977a6b4fdf47930b8f9",
      "placeholder": "​",
      "style": "IPY_MODEL_42663a30a68043f2978fcfe3712b84fa",
      "value": " 570/570 [00:00&lt;00:00, 22.1kB/s]"
     }
    },
    "a2f96386fe184f38ad74700130141290": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a475b7c855bf45b48dad5cfd7f8814a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5d2592adfda4df9b6939e9a30a4c913": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abe66687a433455d96ef6ea193d1993c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5490e6b9a124714adba145ca2bb85f4",
      "max": 570,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b606de690798410ab480bdb5d56fca70",
      "value": 570
     }
    },
    "acf26465e1aa44239c3cec382b4b33d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b606de690798410ab480bdb5d56fca70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b68255e8b7164376b8444cb197862910": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9001aebde3d4663b3b29a7bddef3ae5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccb325512a7344dab2328cd4267dd1c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acf26465e1aa44239c3cec382b4b33d5",
      "placeholder": "​",
      "style": "IPY_MODEL_fc1b5f1cf144419b8b5dd4dc3dc2af4b",
      "value": " 226k/226k [00:00&lt;00:00, 2.94MB/s]"
     }
    },
    "d34174ad7a414212a62a84a386ab0f70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d4d620025a2149cf84154718f0f3a889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d5490e6b9a124714adba145ca2bb85f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de70481d4882465ebe6f313940b9a093": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df818c9657b343dc8cd3aa7a1a23e991": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_53384bab4052401a93010f763b6f5c64",
      "placeholder": "​",
      "style": "IPY_MODEL_144981735bd14cbf86592f9842a8c0f1",
      "value": "Downloading: 100%"
     }
    },
    "e0753dfa37964692b61cfa3a4fe9f697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1b06acc73bb4aa9b5a785674146805b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee4355137e1e40a49bf6af5d56ecad6b",
       "IPY_MODEL_abe66687a433455d96ef6ea193d1993c",
       "IPY_MODEL_a2c4ea5c01a64723ab2f415b2bb8ba57"
      ],
      "layout": "IPY_MODEL_99cfd3bb5d534331a223d5faa93b244f"
     }
    },
    "e1c50261bef24853bd815f614d783dd2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9123e6db17fb418280d4390c2b397ea7",
       "IPY_MODEL_6cc45c3a26b84c77ae8dc1bc84a7079a",
       "IPY_MODEL_8f1462b0b13c436abdc3d65b77bd7d21"
      ],
      "layout": "IPY_MODEL_716d7f32b2f143108e4cfc6aff8d3f9b"
     }
    },
    "ee22a71cb96549b09ec6296ef758fb08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee4355137e1e40a49bf6af5d56ecad6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a5d2592adfda4df9b6939e9a30a4c913",
      "placeholder": "​",
      "style": "IPY_MODEL_48bf8cc47df04c128bf39221d8dbd69a",
      "value": "Downloading: 100%"
     }
    },
    "fa1784d71c9c48ad844309aceda59def": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e7bb7cf4f694ab9aaf1d1b8e290f368",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0c00484cabf84e2eb36826f4b7d1aa6d",
      "value": 28
     }
    },
    "fc1b5f1cf144419b8b5dd4dc3dc2af4b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
